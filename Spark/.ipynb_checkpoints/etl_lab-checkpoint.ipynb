{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec31cb08",
   "metadata": {},
   "source": [
    "# ETL Using Apache Spark\n",
    "\n",
    "**Estimated time needed:** 20 minutes\n",
    "\n",
    "The purpose of this lab is to show you how to pull data from a source system, transform it and store it back to the target system.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "1.  Pull data from the HMP dataset.\n",
    "2.  Create a Spark Dataframe from the raw data.\n",
    "3.  Write the Dataframe to CSV.\n",
    "4.  Convert the CSV data to PARQUET.\n",
    "5.  Condense PARQUET to a single file.\n",
    "6.  Upload the Parquet file to Cloud Object Store\n",
    "\n",
    "But don’t be scared, we provide you with a set of sample notebooks you can modify and hook together. The library where you can get the notebooks from is called CLAIMED – the Component Library for AI, Machine Learning, ETL, and Data Science and is an open-source library available on the [CLAIMED repo](https://github.com/IBM/claimed/tree/master/component-library).\n",
    "\n",
    "You’ll use [Elyra – a JupyterLab extension](https://elyra.readthedocs.io/en/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01) for editing the notebooks; and, if you like you can use the [pipeline editor](https://elyra.readthedocs.io/en/latest/getting_started/overview.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01#ai-pipelines-visual-editor) of Elyra to visually join them into a data pipeline.\n",
    "\n",
    "Elyra is the foundation of the [IBM Watson Studio Pipelines](https://medium.com/ibm-data-ai/automating-the-ai-lifecycle-with-ibm-watson-studio-orchestration-flow-4450f1d725d6?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01) tool which can be used in the cloud. Feel free to give it a shot as well to experience how a business user would do the job.\n",
    "\n",
    "We’ll use the HMP dataset. The dataset is publically available [here](https://github.com/wchill/HMP_Dataset).\n",
    "\n",
    "In previous generation BigData systems, HDFS was the core data store. Nowadays, S3 compatible Cloud Object Storage (COS) is the de-facto standard across clouds and also starts to get traction in local data centers (Ceph, Minio).\n",
    "\n",
    "So let’s get started with the lab!\n",
    "\n",
    "## Exercise 1 : Import the CLAIMED library to JupyterLab\n",
    "\n",
    "1.  In a separate browser tab, please open the CLAIMED component library: [GitHub Link](https://github.com/IBM/claimed/tree/master/component-library)\n",
    "2.  Don’t hesitate to give us a star (1) :), then please click the Fork button. (2)\n",
    "\n",
    "(Taking this action enables you to work using your own copy - you then can replace the link to your fork below in the next cell)\n",
    "\n",
    "![Elyra Github repo](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp4.png)\n",
    "\n",
    "Now let's clone (download) the library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -Rf claimed\n",
    "git clone https://github.com/IBM/claimed.git\n",
    "cd claimed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cddbaca",
   "metadata": {},
   "source": [
    "Congratulations, you’ve successfully imported the component library.\n",
    "\n",
    "## Exercise 2 : Explore the CLAIMED component library\n",
    "\n",
    "Now it’s time to familiarize yourself a bit with some components (Jupyter Notebooks) in the component library.\n",
    "\n",
    "1.  In JuypterLab, please go to the file explorer (1), double-click on folder \"claimed\" and then double click on the folder “component-library” (2)\n",
    "\n",
    "![jupyerlab exploration](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp7.png)\n",
    "\n",
    "2.  Please open the folder called “transform” (1) and open the notebook called spark-csv-to-parquet (2). The prefix “spark” indicates that the notebook is using Apache Spark to perform its task. From the name you can deduce that this component transforms a file from “parquet” to “csv” format. Each notebook starts with a title and description of what it is supposed to do, followed by commands to install library dependencies. Then, a set of parameters this notebook accepts is provided (3), followed by an actual implementation of pulling those parameters from the environment (4). Finally, the actual task is implemented in source code (5).\n",
    "\n",
    "![jupyerlab exploration](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp8.png)\n",
    "\n",
    "3.  Please explore the other components in the library and look at how they are implemented. They serve as cookie-cutters, or examples, for an abundance of daily data science tasks and hopefully you can learn from them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b9a109",
   "metadata": {},
   "source": [
    "## Exercise 3 : Start the ETL process\n",
    "\n",
    "### Task A : Pull the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb402770",
   "metadata": {},
   "source": [
    "Now we need to pull data from a remote github repository and convert it to CSV.\n",
    "You can open the notebook called *input-hmp.ipynb* in folder *claimed/component-library/input* and run\n",
    "each cell top down, one by one. Alternatively, the following cell is doing the job as well.\n",
    "If you run the code by executing the cell below, please open the notebook in parallel and\n",
    "investigate how it was implemented.\n",
    "\n",
    "Please note, that that way we can pass a parameter called *sample* which reduces data size and processing\n",
    "time. Sampling is often done on \"Non Big Data\" Systems, but in Apache Spark this is not necessary most of the\n",
    "times because Apache Spark can handle any amount of data - so sampling is done here only for saving\n",
    "time during the lab.\n",
    "\n",
    "Note: A cell which creates a lot of output can be switched into scrolling mode. Just right-click into the output canvas (1) and click on “Enable Scrolling for Outputs” (2)\n",
    "\n",
    "![creating a pipeline](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp10.png)\n",
    "\n",
    "3.  After quite some time you should see a folder called “data.csv” in the “data” directory you’ve previously created. Please note that Apache Spark always creates folders containing individual files, one per partition. This is not a problem because Spark doesn’t distinguish between files and folders and threats folders as they were files. (The only way to get a file is to repartition the data frame to one and extract/rename the file inside the folder df = df.repartition(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75c2fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ipython ./claimed/component-library/input/input-hmp.ipynb data_dir=./data/ sample=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d60a9",
   "metadata": {},
   "source": [
    "### Task B : Convert from CSV to PARQUET\n",
    "\n",
    "CSV isn't a memory and storage efficient file format - the same holds for JSON by the way. Therefore,\n",
    "formats like [Avro](https://en.wikipedia.org/wiki/Apache_Avro?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01), [Parquet](https://en.wikipedia.org/wiki/Apache_Parquet?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01), [ORC](https://en.wikipedia.org/wiki/Apache_ORC?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01) or [RCFile](https://en.wikipedia.org/wiki/RCFile?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01) emerged. Therefore we convert the CSV data to PARQUET before uploading it to Cloud Object Store.\n",
    "Again, [CLAIMED](https://github.com/IBM/claimed/tree/master/component-library) provides a component as jupyter notebook\n",
    "exactly doing that. Please explore the notebook under *component-library/transform/spark-csv-to-parquet.ipynb*. You can either directly run the notebook or execute the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ipython ./claimed/component-library/transform/spark-csv-to-parquet.ipynb data_dir=./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de393b9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task C: Condense parquet file\n",
    "\n",
    "As mentioned before - Apache Spark creates folders instead of files - in the folder, besides some status information, you will see that one sub-file per partition is written. This is handy for parallel processing but sometimes we\n",
    "want all data to be self-contained in a single file. Therefore, the \"spark-condense-parquet.ipynb\" component\n",
    "in the *./claimed/component-library/transform/* folder does this job. Let's execute it using the cell below, but please\n",
    "also open the notebook to see what it is doing - alternatively you can also run the notebook cell by cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1580f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ipython ./claimed/component-library/transform/spark-condense-parquet.ipynb data_dir=./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609512e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task D : Obtain access to Cloud Object Store\n",
    "\n",
    "1.  Please create an account on [IBM Cloud](https://cloud.ibm.com/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01) - it is free, doesn't expire and no credit card is required. You also get 25 GB storage on Cloud Object Store (COS) for free. IBM COS is compatible to S3. So what you learn here will work on all S3 compliant storage system in different clouds and also in your local data center.\n",
    "\n",
    "2.  Once you create and verify your account you can create an S3 compliant Object Storage service by clicking the following [link](https://cloud.ibm.com/objectstorage/create?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01) - please select the lite plan as it includes 25 GB of free storage\n",
    "\n",
    "![Create COS](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/ibm_cos_create.png)\n",
    "\n",
    "3.  You can always go back to the service created by accessing the [IBM Cloud Service Resource List](https://cloud.ibm.com/resources?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01). Just look for an entry in the table under the \"Storage\" category and click on it.\n",
    "\n",
    "![Resource List](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/ibm_resource_list_cos.png)\n",
    "\n",
    "4.  Now please click \"Create Bucket\"\n",
    "\n",
    "![Create Bucket](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/ibm_cos_create_bucket.png)\n",
    "\n",
    "5.  Select the \"Quickly get started\" option by clicking the arrow\n",
    "\n",
    "![Create Bucket\\_2](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/ibm_cos_create_bucket\\_2.png)\n",
    "\n",
    "6.  Click on \"next\" twice, then click \"View Bucket Configuration\"\n",
    "\n",
    "7.  Here, please note the bucket name, and the location, then click \"Service Credentials\"\n",
    "\n",
    "![Create SC](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/ibm_cos_location_sc.png)\n",
    "\n",
    "8.  Click on \"New Credential\", then under \"Advanced Options\", please ensure that \"Include HMAC Credential\" is active. Then click \"Add\"\n",
    "\n",
    "![hmac](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/hmac.png)\n",
    "\n",
    "9.  Please open the newly created credentials, an in the \"cos_hmac_keys\" section, please note down \"access_key_id\" and \"secret_access_key\"\n",
    "\n",
    "![sc](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/sc.png)\n",
    "\n",
    "10. Click on \"Endpoints\", then select the appropriate location you've obtained four steps before. That way the correct public endpoint is displayed, please note it somewhere for future use.\n",
    "\n",
    "![endpoint](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/endpoint.png)\n",
    "\n",
    "Congratulations! You've obtained all information to store your date to Cloud Object Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa12d8",
   "metadata": {},
   "source": [
    "### Task E: Upload file to Cloud Object Storage\n",
    "\n",
    "Now it's time to use the data you've gathered from task B and fill in the following variables in the next cell.\n",
    "Then please run the cell. This will upload the parquet file you've created to Cloud Object Store where is it\n",
    "stored at the lowest possible cost and made available to others.\n",
    "\n",
    "<span style=\"color:red\">Please note: The endpoint you need to set is not the \"endpoints\" from the service credentials section but from Task D Step 11</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77151a1",
   "metadata": {},
   "source": [
    "> Note :After executing the below code, if you  get an error as:\n",
    "\n",
    "```\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. \n",
    "\n",
    "This behaviour is the source of the following dependency conflicts.aiobotocore 1.4.1 requires botocore<1.20.107,>=1.20.106,\n",
    "but you have botocore 1.10.62 which is incompatible.\n",
    "```\n",
    "\n",
    "Kindly ignore the error and proceed to **TaskF** .Your file will be deployed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export access_key_id='access_key_id=<your_access_key_id_goes_here>'\n",
    "export secret_access_key='secret_access_key=<your_secret_access_key_goes_here>'\n",
    "export endpoint='endpoint=https://<your_endpoint_goes_here>'\n",
    "export bucket_name='bucket_name=<your_cos_bucket_name_goes_here>'\n",
    "export source_file='source_file=data_condensed.parquet'\n",
    "export destination_file='destination_file=data.parquet'\n",
    "export data_dir='data_dir=./data/'\n",
    "ipython ./claimed/component-library/output/upload-to-cos.ipynb $access_key_id $secret_access_key $endpoint $bucket_name $source_file $destination_file $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d52a39",
   "metadata": {},
   "source": [
    "### Task F: Double-check if the file arrived at its destination\n",
    "\n",
    "To make sure everything worked as designed (actually an unnecessary step if you haven't received any errors)\n",
    "we open the IBM Cloud user interface. Please open the bucket contents as you've learned in Task C and check if\n",
    "the file arrived as shown below.\n",
    "\n",
    "![cos_contents](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/check_cos.png)\n",
    "\n",
    "Congratulations, this concludes this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a53f1d1",
   "metadata": {},
   "source": [
    "### Task F: (Optional) Pull another file and upload it as well to COS\n",
    "\n",
    "To make things easier let's first flush the data dir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf ./claimed/data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6082a08",
   "metadata": {},
   "source": [
    "Now it's time for you to pull some other data set. You can choose anything you like but you can also use\n",
    "the \"input-pardata.ipynb\" notebook in folder \"./component-library/input\". This component pulls the JFK\n",
    "weather data set as data.csv. From there, please convert this CSV file to PARQUET, condense it and upload\n",
    "it to Object Storage as \"jfk_data.parquet\". You can either directly open the notebooks, run it inline this\n",
    "notebook below or even create a pipeline using the [Elyra Pipeline editor](https://youtu.be/0SWRMnhIZMA?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01&t=1810).\n",
    "\n",
    "We have provided an inline solution for you below in case you get stuck.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fe49f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ipython ./claimed/component-library/input/input-pardata.ipynb data_dir=./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913dfc1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ipython ./claimed/component-library/transform/spark-csv-to-parquet.ipynb data_dir=./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef44998",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ipython ./claumed/component-library/transform/spark-condense-parquet.ipynb data_dir=./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fb6b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "export access_key_id='access_key_id=<your_access_key_id_goes_here>'\n",
    "export secret_access_key='secret_access_key=<your_secret_access_key_goes_here>'\n",
    "export endpoint='endpoint=https://<your_endpoint_goes_here>'\n",
    "export bucket_name='bucket_name=<your_cos_bucket_name_goes_here>'\n",
    "export source_file='source_file=data_condensed.parquet'\n",
    "export destination_file='destination_file=jfk_data.parquet'\n",
    "export data_dir='data_dir=./data/'\n",
    "ipython ./claimed/component-library/output/upload-to-cos.ipynb $access_key_id $secret_access_key $endpoint $bucket_name $source_file $destination_file $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af52da4",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "\n",
    "[Romeo Kienzler](https://ibm.com/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01)\n",
    "\n",
    "[Karthik Muthuraman](https://ibm.com/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01)\n",
    "\n",
    "## Changelog\n",
    "\n",
    "| Date       | Version | Changed by         | Change Description       |\n",
    "| ---------- | ------- | ------------------ | ------------------------ |\n",
    "| 2021-05-12 | 0.1     | Romeo Kienzler     | Initial version created  |\n",
    "| 2021-06-22 | 1.0     | Karthik Muthuraman | First editorial pass     |\n",
    "| 2021-05-12 | 1.1     | Romeo Kienzler     | Adjusted for API changes |\n",
    "| 2021-05-26 | 1.2     | Romeo Kienzler     | Moved to jupyter lab     |\n",
    "| 2021-05-26 | 1.3     | Romeo Kienzler     | Add optional task        |\n",
    "| 2021-12-06 | 1.4     | Lakshmi Holla      | Added the warning note   |\n",
    "|            |         |                    |                          |\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2021. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
